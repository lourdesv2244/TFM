![imagen](https://github.com/user-attachments/assets/725e914e-8f9f-41bb-af7d-c9e8c92e83a8)


# TFM: ClasificaciÃ³n automÃ¡tica de tickets de solicitudes en Ã¡rea de Operaciones D&A
Sistema de clasificaciÃ³n y asignaciÃ³n automÃ¡tica de tickets recibidos en ServiceNow y extraidos de forma manual para ser atendidos por el equipo de Operaciones del Ã¡rea de Data & Analytics MAZ. Este sistema se desarrollÃ³ en el marco del Trabajo de Fin de MÃ¡ster en Ciencia de Datos e Inteligencia Artificial (curso 2024-2025)

# Resumen
El presente Trabajo de Fin de MÃ¡ster (TFM) tiene como objetivo entregar un Producto MÃ­nimo Viable (MVP) que consiste en un prototipo inicial para la clasificaciÃ³n y asignaciÃ³n automÃ¡tica de tickets tÃ©cnicos en el equipo de soporte de Data & Analytics MAZ. Esta propuesta responde a la creciente demanda de soluciones operativas que permitan gestionar de manera eficiente el volumen de solicitudes recibidas, caracterizadas por un alto grado de complejidad tÃ©cnica y una nomenclatura especÃ­fica del dominio, lo que exige una clasificaciÃ³n precisa y una asignaciÃ³n Ã³ptima a agentes especializados.

La metodologÃ­a se basÃ³ en la combinaciÃ³n de tÃ©cnicas de aprendizaje automÃ¡tico supervisado y procesamiento del lenguaje natural. Inicialmente, se realizÃ³ una recolecciÃ³n y etiquetado manual de 500 tickets reales, distribuidos en seis categorÃ­as definidas por expertos. Posteriormente, se implementÃ³ un preprocesamiento lingÃ¼Ã­stico especializado, con normalizaciÃ³n, lematizaciÃ³n y conservaciÃ³n de tÃ©rminos tÃ©cnicos clave. Se probaron diversos algoritmos tradicionales (Random Forest, SVM, Gradient Boosting), asÃ­ como una red neuronal profunda y un modelo BERT ajustado mediante fine-tuning. A partir de este Ãºltimo se desarrollÃ³ BERT con tokens, una versiÃ³n optimizada que incorpora tokens especiales, capa de atenciÃ³n personalizada y generaciÃ³n de explicaciones automÃ¡ticas.

Los resultados muestran que BERT con tokens alcanza una precisiÃ³n del 92,1â€¯% en clasificaciÃ³n tÃ©cnica, mejorando en mÃ¡s de 5 puntos porcentuales respecto a BERT estÃ¡ndar (86,7â€¯%) y acercÃ¡ndose al rendimiento de los modelos tradicionales mÃ¡s precisos (93,4â€¯%), con la ventaja aÃ±adida de ofrecer explicaciones interpretables sobre cada decisiÃ³n. Finalmente, el sistema se integrÃ³ en un flujo automatizado que no solo clasifica el ticket, sino que asigna dinÃ¡micamente al agente Ã³ptimo en funciÃ³n de su especialidad y carga de trabajo. Se concluye que la arquitectura propuesta representa una soluciÃ³n robusta, explicable y lista para su incorporaciÃ³n al proceso de soporte tÃ©cnico de D&A.

Keywords: Aprendizaje automÃ¡tico supervisado, Procesamiento del lenguaje natural (PLN), Red neuronal profunda, BERT, Fine-tuning, BERT con tokens, Tokens especiales, ClasificaciÃ³n y asignaciÃ³n automÃ¡tica, Modelos tradicionales (Random Forest, SVM, Gradient Boosting). Producto MÃ­nimo Viable (MVP), Prototipo inicial, RecolecciÃ³n de tickets reales, Etiquetado manual, Preprocesamiento lingÃ¼Ã­stico, NormalizaciÃ³n, LematizaciÃ³n.

Este repositorio contiene el **Producto MÃ­nimo Viable (MVP)** de una herramienta automÃ¡tica para la clasificaciÃ³n y asignaciÃ³n de tickets tÃ©cnicos, desarrollada en Python y diseÃ±ada para integrarse fÃ¡cilmente con flujos basados en Excel.


# ğŸ“‚ Estructura del repositorio
La estructura de carpetas con la documentaciÃ³n de Github es la presentada a continuaciÃ³n:

        01 DocumentaciÃ³n Github
         â””â”€â”€ data
         Â Â   â”œâ”€â”€ Mensajes_Clasificados_Manual.xlsx
         Â Â   â”œâ”€â”€ data_entrada.xlsx
         Â Â   â””â”€â”€ resultado_BERT_tokens.xlsx
         â””â”€â”€ Instalacion
         Â Â   â”œâ”€â”€ check_system.py
         Â Â   â””â”€â”€ modelo_bert_tokens
                 â”œâ”€â”€ added_tokens.json
                 â”œâ”€â”€ bert2_weights.pt
                 â”œâ”€â”€ label_encoder.pkl
                 â”œâ”€â”€ special_tokens_map.json
                 â””â”€â”€ vocab.txt
         â””â”€â”€ codigo
         Â Â   â””â”€â”€ Clasificacion Mensajes v2.ipynb
                 
         Â Â     

Se detalla a continuaciÃ³n el contenido de cada fichero:

- added_tokens.json: Contiene el mapeo de los tokens personalizados que se han aÃ±adido al vocabulario original de BERT. Cada clave es el nuevo token (p. ej. nombres de herramientas o acrÃ³nimos del dominio) y su valor es el Ã­ndice (entero) que ocupa dentro de la nueva tabla de vocabulario. Se usa en el BertTokenizer para que reconozca y trate correctamente estos tokens durante la tokenizaciÃ³n.
- special_tokens_map.json: Define el conjunto de â€œtokens especialesâ€ (por ejemplo [CLS], [SEP], [PAD], [UNK], etc.) y, opcionalmente, otros tokens propios ([AADS], [DELTA],â€¦) que se consideraron relevantes. Mapea cada tipo de token especial a la cadena que lo representa, de modo que el tokenizer sepa insertarlos o sustituirlos de forma coherente en el texto de entrada.
- bert2_weights.pt: Contiene el state_dict completo del modelo BertForSequenceClassification tras el proceso de fine-tuning. Incluye los pesos de todas las capas Transformer, la capa de clasificaciÃ³n y cualquier mÃ³dulo aÃ±adido (p. ej. capa de atenciÃ³n extra). Se carga con model.load_state_dict(torch.load("bert2_weights.pt")) para restaurar el modelo en memoria.
- VOCAB.TXT: Lista el vocabulario completo que utiliza el tokenizer: empieza con los tokens originales de bert-base-uncased y, a continuaciÃ³n, incorpora en las Ãºltimas lÃ­neas los tokens introducidos (los mismos que se reflejan en added_tokens.json). El orden y la posiciÃ³n de cada lÃ­nea corresponden al Ã­ndice que el tokenizer asigna en los tensores de entrada.
- label_encoder.pkl: Almacena el objeto LabelEncoder de scikit-learn que traduce las categorÃ­as de texto (p. ej. "AADS Group - Acceso", "Delta Share") a sus cÃ³digos numÃ©ricos internos (0, 1, â€¦). â€¢ Se recupera con label_encoder = pickle.load(open("label_encoder.pkl","rb")) para decodificar las predicciones numÃ©ricas del modelo de vuelta a su nombre de categorÃ­a original.


# ğŸ“ˆ Ejemplo de flujo

   1. Lectura y preprocesamiento â€“ Limpieza de texto, lematizaciÃ³n y extracciÃ³n de entidades con spaCy/CoreNLP.
   2. ClasificaciÃ³n â€“ Transformer fine-tuned que devuelve categorÃ­as (multietiqueta si estÃ¡ configurado).
   3. AsignaciÃ³n â€“ Algoritmo basado en carga de trabajo, especialidad y reglas definidas en config.yaml.
   4. ExportaciÃ³n â€“ GeneraciÃ³n de un archivo Excel 


# ğŸ“š Recursos adicionales

   * Stanford CoreNLP â€“ Para extracciÃ³n de entidades y anÃ¡lisis sintÃ¡ctico.
   * Hugging Face Transformers â€“ Para modelos BERT, GPT y fine-tuning.
   * spaCy â€“ Para tokenizaciÃ³n y procesamiento rÃ¡pido en Python.


# ğŸ”§ InstalaciÃ³n
## Paso a Paso: Cargar y Verificar el Modelo

A continuaciÃ³n se describe cÃ³mo cargar el sistema de clasificaciÃ³n y comprobar que todo estÃ¡ listo para realizar predicciones.

### 1. Clona el repositorio  

        git clone https://github.com/tu-usuario/ticket-mvp.git
        cd ticket-mvp

### 2. Crea y activa un entorno virtual  

        python3 -m venv venv
        source venv/bin/activate       # macOS / Linux
        venv\Scripts\activate          # Windows

### 3. Instala las dependencias  

        pip install -r requirements.txt

### 4. Carga el modelo y muestra el estado del sistema
Abre un intÃ©rprete de Python o crea un script (check_system.py) con el siguiente contenido:  

        pip install -r Requirements.txt

### 5. Carga el modelo y muestra el estado del sistema 
        
        python check_system.py


### 6.  Interpreta la salida

- Tokenizer cargadoâ€¦: nÃºmero de tokens en el vocabulario.
- LabelEncoderâ€¦: lista de categorÃ­as que reconoce el modelo.
- Modo evaluaciÃ³n: True indica que el modelo estÃ¡ listo para predecir (sin activar gradientes).

# ğŸ‘¤ Autor
Lourdes VillafaÃ±a

  

