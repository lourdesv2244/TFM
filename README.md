# TFM: ClasificaciÃ³n automÃ¡tica de tickets de solicitudes en Ã¡rea de Operaciones D&A
Sistema de clasificaciÃ³n y asignaciÃ³n automÃ¡tica de tickets en ServiceNow para el Ã¡rea de Data & Analytics de MAZ. Este sistema se desarrollÃ³ en el marco del Trabajo de Fin de MÃ¡ster en Ciencia de Datos e Inteligencia Artificial (curso 2024-2025)

# Resumen
El presente Trabajo de Fin de MÃ¡ster (TFM) tiene como objetivo entregar un Producto MÃ­nimo Viable (MVP) que consiste en un prototipo inicial para la clasificaciÃ³n y asignaciÃ³n automÃ¡tica de tickets tÃ©cnicos en el equipo de soporte de Data & Analytics MAZ. Esta propuesta responde a la creciente demanda de soluciones operativas que permitan gestionar de manera eficiente el volumen de solicitudes recibidas, caracterizadas por un alto grado de complejidad tÃ©cnica y una nomenclatura especÃ­fica del dominio, lo que exige una clasificaciÃ³n precisa y una asignaciÃ³n Ã³ptima a agentes especializados.

La metodologÃ­a se basÃ³ en la combinaciÃ³n de tÃ©cnicas de aprendizaje automÃ¡tico supervisado y procesamiento del lenguaje natural. Inicialmente, se realizÃ³ una recolecciÃ³n y etiquetado manual de 500 tickets reales, distribuidos en seis categorÃ­as definidas por expertos. Posteriormente, se implementÃ³ un preprocesamiento lingÃ¼Ã­stico especializado, con normalizaciÃ³n, lematizaciÃ³n y conservaciÃ³n de tÃ©rminos tÃ©cnicos clave. Se probaron diversos algoritmos tradicionales (Random Forest, SVM, Gradient Boosting), asÃ­ como una red neuronal profunda y un modelo BERT ajustado mediante fine-tuning. A partir de este Ãºltimo se desarrollÃ³ BERT con tokens, una versiÃ³n optimizada que incorpora tokens especiales, capa de atenciÃ³n personalizada y generaciÃ³n de explicaciones automÃ¡ticas.

Los resultados muestran que BERT con tokens alcanza una precisiÃ³n del 92,1â€¯% en clasificaciÃ³n tÃ©cnica, mejorando en mÃ¡s de 5 puntos porcentuales respecto a BERT estÃ¡ndar (86,7â€¯%) y acercÃ¡ndose al rendimiento de los modelos tradicionales mÃ¡s precisos (93,4â€¯%), con la ventaja aÃ±adida de ofrecer explicaciones interpretables sobre cada decisiÃ³n. Finalmente, el sistema se integrÃ³ en un flujo automatizado que no solo clasifica el ticket, sino que asigna dinÃ¡micamente al agente Ã³ptimo en funciÃ³n de su especialidad y carga de trabajo. Se concluye que la arquitectura propuesta representa una soluciÃ³n robusta, explicable y lista para su incorporaciÃ³n al proceso de soporte tÃ©cnico de D&A.

Keywords: Aprendizaje automÃ¡tico supervisado, Procesamiento del lenguaje natural (PLN), Red neuronal profunda, BERT, Fine-tuning, BERT con tokens, Tokens especiales, ClasificaciÃ³n y asignaciÃ³n automÃ¡tica, Modelos tradicionales (Random Forest, SVM, Gradient Boosting). Producto MÃ­nimo Viable (MVP), Prototipo inicial, RecolecciÃ³n de tickets reales, Etiquetado manual, Preprocesamiento lingÃ¼Ã­stico, NormalizaciÃ³n, LematizaciÃ³n.

Este repositorio contiene el **Producto MÃ­nimo Viable (MVP)** de una herramienta automÃ¡tica para la clasificaciÃ³n y asignaciÃ³n de tickets tÃ©cnicos, desarrollada en Python y diseÃ±ada para integrarse fÃ¡cilmente con flujos basados en Excel o plataformas como ServiceNow.


La estructura de carpetas con la documentaciÃ³n de Github es la presentada a continuaciÃ³n:

        01 DocumentaciÃ³n Github
         â””â”€â”€ 00_Codigo
         Â Â   â”œâ”€â”€ DOCUMENTACIÃ“N GITHUB.docx
         Â Â   â”œâ”€â”€ ejemplo (1).py
         Â Â   â”œâ”€â”€ Modelo_binario_ (1) (1).ipynb
         Â Â   â”œâ”€â”€ obtener_caracteristicas (1).py
          Â Â  â””â”€â”€ Recursos-20231027T110710Z-001 (1).zip

Se detalla a continuaciÃ³n el contenido de cada fichero:

- DOCUMENTACIÃ“N GITHUB.docx:






## ğŸ“‚ Estructura del repositorio
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_tickets.xlsx         # Ejemplo de archivo de entrada
â”œâ”€â”€ models/
â”‚   â””â”€â”€ tokenizer/                  # Tokenizer de Transformer (p. ej. BERT)
â”‚   â””â”€â”€ classifier/                 # Pesos del modelo de clasificaciÃ³n
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classify_assign.py          # Script principal de clasificaciÃ³n y asignaciÃ³n
â”‚   â”œâ”€â”€ preprocessing.py            # MÃ³dulo de preprocesamiento (CoreNLP + spaCy)
â”‚   â”œâ”€â”€ model.py                    # DefiniciÃ³n del pipeline de Transformer
â”‚   â””â”€â”€ utils.py                    # Funciones auxiliares (I/O, Excel, configuraciÃ³n)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb           # AnÃ¡lisis exploratorio y pruebas interactivas
â”œâ”€â”€ requirements.txt                # Dependencias del proyecto
â””â”€â”€ README.md                       # DocumentaciÃ³n principal




## ğŸš€ InstalaciÃ³n

1. **Clona el repositorio**
   ```bash
   git clone https://github.com/tu-usuario/ticket-mvp.git
   cd ticket-mvp

2. **Crea un entorno virtual (recomendado)**
   ```bash
   python3.11 -m venv .venv
   source .venv/bin/activate    # macOS/Linux
   .venv\Scripts\activate       # Windows

3. **Instala las dependencias**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt

4. **Descarga el modelo preentrenado Copia los pesos del tokenizer y el clasificador en models/tokenizer/ y models/classifier/ respectivamente. Si usas un modelo de Hugging Face, por ejemplo BERT fine-tuned:**
   ```bash
   mkdir -p models/tokenizer models/classifier
   cd models/tokenizer
   git clone https://huggingface.co/tu-usuario/bert-tokenizer.git .
   cd ../classifier
   git clone https://huggingface.co/tu-usuario/bert-classifier.git .



## ğŸ¯ Uso
python src/classify_assign.py \
  --input data/sample_tickets.xlsx \
  --output results/assigned_tickets.xlsx \
  --config src/config.yaml

  ```bash
   mkdir -p models/tokenizer models/classifier
   cd models/tokenizer
   git clone https://huggingface.co/tu-usuario/bert-tokenizer.git .
   cd ../classifier
   git clone https://huggingface.co/tu-usuario/bert-classifier.git .
   python src/classify_assign.py \
        --input data/sample_tickets.xlsx \
        --output results/assigned_tickets.xlsx \
        --config src/config.yaml


* --input: Ruta al archivo Excel que contiene los tickets sin procesar.
* --output: Ruta donde se generarÃ¡ el archivo con categorÃ­a y agente asignado.
* --config: Opcional, archivo YAML con parÃ¡metros de umbral, reglas de asignaciÃ³n y estructura de columnas.



âš™ï¸ ConfiguraciÃ³n

El archivo src/config.yaml permite ajustar:

 ```bash
 preprocessing:
    nlp_pipeline: "spacy"    # "spacy" o "corenlp"
    lemmatize: true

 classification:
    model_path: "../models/classifier"
    threshold: 0.5
   
 assignment:
    strategy: "load_balance"  # "load_balance", "skill_match"
    agent_metadata: "../data/agents.json"


ğŸ“ˆ Ejemplo de flujo

   1. Lectura y preprocesamiento â€“ Limpieza de texto, lematizaciÃ³n y extracciÃ³n de entidades con spaCy/CoreNLP.
   2. ClasificaciÃ³n â€“ Transformer fine-tuned que devuelve categorÃ­as (multietiqueta si estÃ¡ configurado).
   3. AsignaciÃ³n â€“ Algoritmo basado en carga de trabajo, especialidad y reglas definidas en config.yaml.
   4. ExportaciÃ³n â€“ GeneraciÃ³n de un archivo Excel con columnas:
       * Ticket ID
       * Texto original
       * CategorÃ­as asignadas
       * Agente sugerido

     

ğŸ“š Recursos adicionales

   * Stanford CoreNLP â€“ Para extracciÃ³n de entidades y anÃ¡lisis sintÃ¡ctico.
   * Hugging Face Transformers â€“ Para modelos BERT, GPT y fine-tuning.
   * spaCy â€“ Para tokenizaciÃ³n y procesamiento rÃ¡pido en Python.

     

ğŸ¤ Contribuciones
1. **Haz un fork del repositorio**
2. **Crea una rama con tu mejora:**
   ```bash
   git checkout -b feature/nueva-caracteristica
3. **Haz tus commits:**
   ```bash
   git commit -m "AÃ±ade X funcionalidad"
4. **EnvÃ­a un pull request.**


   
ğŸ“„ Licencia
Este proyecto estÃ¡ bajo la licencia MIT. 
