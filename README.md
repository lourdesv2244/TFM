![imagen](https://github.com/user-attachments/assets/725e914e-8f9f-41bb-af7d-c9e8c92e83a8)


# TFM: Clasificaci√≥n autom√°tica de tickets de solicitudes en √°rea de Operaciones D&A
Sistema de clasificaci√≥n y asignaci√≥n autom√°tica de tickets recibidos en ServiceNow y extraidos de forma manual para ser atendidos por el equipo de Operaciones del √°rea de Data & Analytics MAZ. Este sistema se desarroll√≥ en el marco del Trabajo de Fin de M√°ster en Ciencia de Datos e Inteligencia Artificial (curso 2024-2025)

# Resumen
El presente Trabajo de Fin de M√°ster (TFM) tiene como objetivo entregar un Producto M√≠nimo Viable (MVP) que consiste en un prototipo inicial para la clasificaci√≥n y asignaci√≥n autom√°tica de tickets t√©cnicos en el equipo de soporte de Data & Analytics MAZ. Esta propuesta responde a la creciente demanda de soluciones operativas que permitan gestionar de manera eficiente el volumen de solicitudes recibidas, caracterizadas por un alto grado de complejidad t√©cnica y una nomenclatura espec√≠fica del dominio, lo que exige una clasificaci√≥n precisa y una asignaci√≥n √≥ptima a agentes especializados.

La metodolog√≠a se bas√≥ en la combinaci√≥n de t√©cnicas de aprendizaje autom√°tico supervisado y procesamiento del lenguaje natural. Inicialmente, se realiz√≥ una recolecci√≥n y etiquetado manual de 500 tickets reales, distribuidos en seis categor√≠as definidas por expertos. Posteriormente, se implement√≥ un preprocesamiento ling√º√≠stico especializado, con normalizaci√≥n, lematizaci√≥n y conservaci√≥n de t√©rminos t√©cnicos clave. Se probaron diversos algoritmos tradicionales (Random Forest, SVM, Gradient Boosting), as√≠ como una red neuronal profunda y un modelo BERT ajustado mediante fine-tuning. A partir de este √∫ltimo se desarroll√≥ BERT con tokens, una versi√≥n optimizada que incorpora tokens especiales, capa de atenci√≥n personalizada y generaci√≥n de explicaciones autom√°ticas.

Los resultados muestran que BERT con tokens alcanza una precisi√≥n del 92,1‚ÄØ% en clasificaci√≥n t√©cnica, mejorando en m√°s de 5 puntos porcentuales respecto a BERT est√°ndar (86,7‚ÄØ%) y acerc√°ndose al rendimiento de los modelos tradicionales m√°s precisos (93,4‚ÄØ%), con la ventaja a√±adida de ofrecer explicaciones interpretables sobre cada decisi√≥n. Finalmente, el sistema se integr√≥ en un flujo automatizado que no solo clasifica el ticket, sino que asigna din√°micamente al agente √≥ptimo en funci√≥n de su especialidad y carga de trabajo. Se concluye que la arquitectura propuesta representa una soluci√≥n robusta, explicable y lista para su incorporaci√≥n al proceso de soporte t√©cnico de D&A.

Keywords: Aprendizaje autom√°tico supervisado, Procesamiento del lenguaje natural (PLN), Red neuronal profunda, BERT, Fine-tuning, BERT con tokens, Tokens especiales, Clasificaci√≥n y asignaci√≥n autom√°tica, Modelos tradicionales (Random Forest, SVM, Gradient Boosting). Producto M√≠nimo Viable (MVP), Prototipo inicial, Recolecci√≥n de tickets reales, Etiquetado manual, Preprocesamiento ling√º√≠stico, Normalizaci√≥n, Lematizaci√≥n.

Este repositorio contiene el **Producto M√≠nimo Viable (MVP)** de una herramienta autom√°tica para la clasificaci√≥n y asignaci√≥n de tickets t√©cnicos, desarrollada en Python y dise√±ada para integrarse f√°cilmente con flujos basados en Excel.


# üìÇ Estructura del repositorio
La estructura de carpetas con la documentaci√≥n de Github es la presentada a continuaci√≥n:

        01 Documentaci√≥n Github
         ‚îî‚îÄ‚îÄ data
         ¬†¬†  ‚îú‚îÄ‚îÄ Mensajes_Clasificados_Manual.xlsx
         ¬†¬†  ‚îú‚îÄ‚îÄ ejemplo (1).py
         ¬†¬†  ‚îú‚îÄ‚îÄ Modelo_binario_ (1) (1).ipynb
         ¬†¬†  ‚îú‚îÄ‚îÄ obtener_caracteristicas (1).py
          ¬†¬† ‚îî‚îÄ‚îÄ Recursos-20231027T110710Z-001 (1).zip
          ‚îî‚îÄ‚îÄ Instalacion
         ¬†¬†  ‚îú‚îÄ‚îÄ check_system.py
         ¬†¬†  ‚îú‚îÄ‚îÄ modelo_bert_tokens
                 ‚îú‚îÄ‚îÄ added_tokens.json
                 ‚îú‚îÄ‚îÄ bert2_weights.pt
                 ‚îú‚îÄ‚îÄ label_encoder.pkl
                 ‚îú‚îÄ‚îÄ special_tokens_map.json
                 ‚îî‚îÄ‚îÄ vocab.txt
                 
         ¬†¬†    

Se detalla a continuaci√≥n el contenido de cada fichero:

- added_tokens.json: Contiene el mapeo de los tokens personalizados que se han a√±adido al vocabulario original de BERT. Cada clave es el nuevo token (p. ej. nombres de herramientas o acr√≥nimos del dominio) y su valor es el √≠ndice (entero) que ocupa dentro de la nueva tabla de vocabulario. Se usa en el BertTokenizer para que reconozca y trate correctamente estos tokens durante la tokenizaci√≥n.
- special_tokens_map.json: Define el conjunto de ‚Äútokens especiales‚Äù (por ejemplo [CLS], [SEP], [PAD], [UNK], etc.) y, opcionalmente, otros tokens propios ([AADS], [DELTA],‚Ä¶) que se consideraron relevantes. Mapea cada tipo de token especial a la cadena que lo representa, de modo que el tokenizer sepa insertarlos o sustituirlos de forma coherente en el texto de entrada.
- bert2_weights.pt: Contiene el state_dict completo del modelo BertForSequenceClassification tras el proceso de fine-tuning. Incluye los pesos de todas las capas Transformer, la capa de clasificaci√≥n y cualquier m√≥dulo a√±adido (p. ej. capa de atenci√≥n extra). Se carga con model.load_state_dict(torch.load("bert2_weights.pt")) para restaurar el modelo en memoria.
- VOCAB.TXT: Lista el vocabulario completo que utiliza el tokenizer: empieza con los tokens originales de bert-base-uncased y, a continuaci√≥n, incorpora en las √∫ltimas l√≠neas los tokens introducidos (los mismos que se reflejan en added_tokens.json). El orden y la posici√≥n de cada l√≠nea corresponden al √≠ndice que el tokenizer asigna en los tensores de entrada.
- label_encoder.pkl: Almacena el objeto LabelEncoder de scikit-learn que traduce las categor√≠as de texto (p. ej. "AADS Group - Acceso", "Delta Share") a sus c√≥digos num√©ricos internos (0, 1, ‚Ä¶). ‚Ä¢ Se recupera con label_encoder = pickle.load(open("label_encoder.pkl","rb")) para decodificar las predicciones num√©ricas del modelo de vuelta a su nombre de categor√≠a original.


# üìà Ejemplo de flujo

   1. Lectura y preprocesamiento ‚Äì Limpieza de texto, lematizaci√≥n y extracci√≥n de entidades con spaCy/CoreNLP.
   2. Clasificaci√≥n ‚Äì Transformer fine-tuned que devuelve categor√≠as (multietiqueta si est√° configurado).
   3. Asignaci√≥n ‚Äì Algoritmo basado en carga de trabajo, especialidad y reglas definidas en config.yaml.
   4. Exportaci√≥n ‚Äì Generaci√≥n de un archivo Excel 


# üìö Recursos adicionales

   * Stanford CoreNLP ‚Äì Para extracci√≥n de entidades y an√°lisis sint√°ctico.
   * Hugging Face Transformers ‚Äì Para modelos BERT, GPT y fine-tuning.
   * spaCy ‚Äì Para tokenizaci√≥n y procesamiento r√°pido en Python.


# Instalaci√≥n
## Paso a Paso: Cargar y Verificar el Modelo

A continuaci√≥n se describe c√≥mo cargar el sistema de clasificaci√≥n y comprobar que todo est√° listo para realizar predicciones.

### 1. Clona el repositorio  

        git clone https://github.com/tu-usuario/ticket-mvp.git
        cd ticket-mvp

### 2. Crea y activa un entorno virtual  

        python3 -m venv venv
        source venv/bin/activate       # macOS / Linux
        venv\Scripts\activate          # Windows

### 3. Instala las dependencias  

        pip install -r requirements.txt

### 4. Carga el modelo y muestra el estado del sistema
Abre un int√©rprete de Python o crea un script (check_system.py) con el siguiente contenido:  

        pip install -r Requirements.txt

### 5. Carga el modelo y muestra el estado del sistema 
        
        python check_system.py


### 6.  Interpreta la salida

- Tokenizer cargado‚Ä¶: n√∫mero de tokens en el vocabulario.
- LabelEncoder‚Ä¶: lista de categor√≠as que reconoce el modelo.
- Modo evaluaci√≥n: True indica que el modelo est√° listo para predecir (sin activar gradientes).

  

